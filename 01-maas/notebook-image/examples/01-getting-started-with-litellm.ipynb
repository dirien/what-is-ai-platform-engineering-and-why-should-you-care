{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Getting Started with LiteLLM on MaaS Platform\n",
    "\n",
    "This notebook demonstrates how to use the LLM models available on your MaaS platform through the LiteLLM proxy.\n",
    "\n",
    "## Prerequisites\n",
    "- Your notebook is connected to the MaaS platform\n",
    "- You have an API key from the MaaS portal\n",
    "- LiteLLM proxy is accessible at the configured endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# The OPENAI_API_BASE is pre-configured to point to LiteLLM\n",
    "# You just need to set your API key from the MaaS portal\n",
    "print(f\"LiteLLM Endpoint: {os.environ.get('OPENAI_API_BASE', 'Not configured')}\")\n",
    "\n",
    "# Set your API key here (get it from the MaaS portal -> API Keys)\n",
    "# os.environ['OPENAI_API_KEY'] = 'sk-your-api-key-here'\n",
    "\n",
    "# Initialize the OpenAI client with LiteLLM endpoint\n",
    "client = OpenAI(\n",
    "    base_url=os.environ.get('OPENAI_API_BASE', 'http://litellm.default.svc.cluster.local:4000'),\n",
    "    api_key=os.environ.get('OPENAI_API_KEY', 'your-api-key')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "models-header",
   "metadata": {},
   "source": [
    "## 2. List Available Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "list-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all available models from LiteLLM\n",
    "try:\n",
    "    models = client.models.list()\n",
    "    print(\"Available Models:\")\n",
    "    print(\"-\" * 50)\n",
    "    for model in models.data:\n",
    "        print(f\"  - {model.id}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error listing models: {e}\")\n",
    "    print(\"Make sure your API key is set correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chat-header",
   "metadata": {},
   "source": [
    "## 3. Basic Chat Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chat-completion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple chat completion example\n",
    "def chat(message, model=\"Qwen/Qwen2.5-7B-Instruct\"):\n",
    "    \"\"\"Send a message to the LLM and get a response.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": message}\n",
    "        ],\n",
    "        max_tokens=500,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Test with a simple question\n",
    "response = chat(\"What is platform engineering? Explain in 2-3 sentences.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-header",
   "metadata": {},
   "source": [
    "## 4. Streaming Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming chat completion for real-time output\n",
    "def chat_stream(message, model=\"Qwen/Qwen2.5-7B-Instruct\"):\n",
    "    \"\"\"Send a message and stream the response.\"\"\"\n",
    "    stream = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": message}\n",
    "        ],\n",
    "        max_tokens=500,\n",
    "        temperature=0.7,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "    print()  # New line at the end\n",
    "\n",
    "# Test streaming\n",
    "print(\"Streaming response:\")\n",
    "chat_stream(\"Write a haiku about Kubernetes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conversation-header",
   "metadata": {},
   "source": [
    "## 5. Multi-turn Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conversation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-turn conversation with context\n",
    "class Conversation:\n",
    "    def __init__(self, model=\"Qwen/Qwen2.5-7B-Instruct\", system_prompt=None):\n",
    "        self.model = model\n",
    "        self.messages = []\n",
    "        if system_prompt:\n",
    "            self.messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    \n",
    "    def chat(self, message):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": message})\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=self.messages,\n",
    "            max_tokens=500,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        assistant_message = response.choices[0].message.content\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "        \n",
    "        return assistant_message\n",
    "\n",
    "# Create a conversation with a system prompt\n",
    "conv = Conversation(system_prompt=\"You are a helpful DevOps assistant specializing in Kubernetes and cloud infrastructure.\")\n",
    "\n",
    "# First message\n",
    "print(\"User: What is a Pod in Kubernetes?\")\n",
    "print(f\"Assistant: {conv.chat('What is a Pod in Kubernetes?')}\")\n",
    "print()\n",
    "\n",
    "# Follow-up question (context is maintained)\n",
    "print(\"User: How is it different from a Deployment?\")\n",
    "print(f\"Assistant: {conv.chat('How is it different from a Deployment?')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "code-gen-header",
   "metadata": {},
   "source": [
    "## 6. Code Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-gen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the LLM to generate code\n",
    "code_prompt = \"\"\"\n",
    "Write a Python function that:\n",
    "1. Takes a list of numbers as input\n",
    "2. Returns a dictionary with statistics: min, max, mean, and standard deviation\n",
    "3. Include type hints and a docstring\n",
    "\n",
    "Only output the code, no explanations.\n",
    "\"\"\"\n",
    "\n",
    "generated_code = chat(code_prompt)\n",
    "print(generated_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-generated",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can copy the generated code here and test it\n",
    "# Example:\n",
    "from typing import List, Dict\n",
    "import statistics\n",
    "\n",
    "def calculate_statistics(numbers: List[float]) -> Dict[str, float]:\n",
    "    \"\"\"Calculate basic statistics for a list of numbers.\"\"\"\n",
    "    return {\n",
    "        \"min\": min(numbers),\n",
    "        \"max\": max(numbers),\n",
    "        \"mean\": statistics.mean(numbers),\n",
    "        \"std_dev\": statistics.stdev(numbers) if len(numbers) > 1 else 0\n",
    "    }\n",
    "\n",
    "# Test it\n",
    "test_numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "print(calculate_statistics(test_numbers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you've learned the basics, you can:\n",
    "\n",
    "1. **Use Jupyter AI Chat**: Click the chat icon in the sidebar to have a conversation with the AI\n",
    "2. **Explore LangChain**: See the `02-langchain-workflows.ipynb` notebook for advanced workflows\n",
    "3. **Build Applications**: Use these patterns to build your own AI-powered applications\n",
    "\n",
    "### Tips\n",
    "- Get your API key from the MaaS portal (API Keys tab)\n",
    "- Check available models on the Models tab\n",
    "- Monitor your usage on the Subscriptions tab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
